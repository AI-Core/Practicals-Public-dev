{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tuning an Image Classifier in Pytorch\n",
    "\n",
    "In this practical, we will fine-tune a pretrained ResNet-18 model to work with a custom image dataset. The task in this case is to train the model to classify photos depending on which of 10 different global cities they were taken in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509daaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title # Run the following cell to download the necessary files for this practical. { display-mode: \"form\" } \n",
    "#@markdown Don't worry about what's in this collapsed cell\n",
    "\n",
    "print('Downloading dataset.py...')\n",
    "!wget https://s3-eu-west-1.amazonaws.com/aicore-portal-public-dev-524288083424/practicals_files/41982379-5961-4188-91d7-22fcb7f1c6ef/dataset.py -q\n",
    "import dataset\n",
    "print('Downloading images.zip...')\n",
    "!wget https://s3-eu-west-1.amazonaws.com/aicore-portal-public-dev-524288083424/practicals_files/41982379-5961-4188-91d7-22fcb7f1c6ef/images.zip -q\n",
    "!unzip images.zip > /dev/null\n",
    "!rm images.zip\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to import the necessary dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from dataset import CitiesDataset\n",
    "from torchvision.models import resnet50\n",
    "from torchvision.models import ResNet50_Weights\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "from torch.utils.data import random_split\n",
    "from torchvision import transforms\n",
    "from torch.optim import lr_scheduler\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, we will define our network. We create a class that inherits from `torch.nn.Module`, and call the `super().__init__()` method to inherit the methods from the parent class. We have also loaded the `ResNet50` model using the pretrained weights.\n",
    "\n",
    "- Add code to set the `grad_required` argument to False for all the `ResNet` layers.\n",
    "- Define a set of three linear layers for the model, assigning them to the variable `linear_layers`. \n",
    "- The output size of the last layer of `ResNet` is 2048.\n",
    "- The middle layer should have input size 256 and output size of 128.\n",
    "- The output layer should have an output size equivalent to the number of classes in the dataset.\n",
    "\n",
    "- Define the forward method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransferLearning(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = resnet50(weights=ResNet50_Weights)\n",
    "        for param in self.layers.parameters():\n",
    "            param.grad_required = False\n",
    "        linear_layers = torch.nn.Sequential(\n",
    "            torch.nn.Linear(2048, 256),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(256, 128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(128, 10),\n",
    "        )\n",
    "        self.layers.fc = linear_layers\n",
    "        # print(self.layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rrritalin/miniconda3/envs/huggingface1/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "size = 128\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(size),\n",
    "    transforms.RandomCrop((size, size), pad_if_needed=True),\n",
    "    transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "dataset = CitiesDataset(transform=transform)\n",
    "model = TransferLearning()\n",
    "\n",
    "# TODO - Split the dataset into train, validation and test sets. The train set should be 70% of dataset length, and the validation and test sets 15% each.\n",
    "# TODO - Create dataloaders for train, validation and test sets. Set batch size to 32, and make sure shuffle=True in the train loader.\n",
    "\n",
    "train_set_len = round(0.7*len(dataset))\n",
    "val_set_len = round(0.15*len(dataset))\n",
    "test_set_len = len(dataset) - val_set_len - train_set_len\n",
    "split_lengths = [train_set_len, val_set_len, test_set_len]\n",
    "# split the data to get validation and test sets\n",
    "train_set, val_set, test_set = random_split(dataset, split_lengths)\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_set, shuffle=True, batch_size=batch_size)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prior to training, it will be interesting to see how the classifier performs straight out of the box. In the box below, pass a single example from the test set to the model, with the model in evaluation mode. Get the prediction and compare it to the real label. How did the model do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction label:  8\n",
      "Prediction category:  Sydney, Australia\n",
      "target label: 0\n",
      "target city  Beijing, China\n"
     ]
    }
   ],
   "source": [
    "features,label=test_set[1]\n",
    "features=features.unsqueeze(0)\n",
    "model.eval()\n",
    "outputs=model(features)\n",
    "\n",
    "dummy, pred = torch.max(outputs, 1)\n",
    "print(\"Prediction label: \", pred.item())\n",
    "class_label = dataset.idx_to_city_name[pred.item()]\n",
    "print(\"Prediction category: \", class_label)\n",
    "\n",
    "print(\"target label:\", label )\n",
    "target_label=dataset.idx_to_city_name[label]\n",
    "print( \"target city \", target_label )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    test_loader,\n",
    "    lr=0.1,\n",
    "    epochs=20,\n",
    "    optimiser=torch.optim.SGD\n",
    "):\n",
    " \n",
    "    writer = SummaryWriter()\n",
    "    # initialise an optimiser\n",
    "    optimiser = optimiser(model.parameters(), lr=lr, weight_decay=0.001)\n",
    "    scheduler = lr_scheduler.MultiStepLR(optimiser, milestones=[5,15], gamma=0.1,verbose=True)\n",
    "    batch_idx = 0\n",
    "    epoch_idx= 0\n",
    "    for epoch in range(epochs):  # for each epoch\n",
    "        # \n",
    "        \n",
    "        print('Epoch:', epoch_idx,'LR:', scheduler.get_lr())\n",
    "        epoch_idx +=1\n",
    "        \n",
    "        for batch in train_loader:  # for each batch in the dataloader\n",
    "            features, labels = batch\n",
    "            prediction = model(features)  # make a prediction\n",
    "            # compare the prediction to the label to calculate the loss (how bad is the model)\n",
    "            loss = F.cross_entropy(prediction, labels)\n",
    "            loss.backward()  # calculate the gradient of the loss with respect to each model parameter\n",
    "            optimiser.step()  # use the optimiser to update the model parameters using those gradients\n",
    "            print(\"Epoch:\", epoch, \"Batch:\", batch_idx,\n",
    "                  \"Loss:\", loss.item())  # log the loss\n",
    "            optimiser.zero_grad()  # zero grad\n",
    "            writer.add_scalar(\"Loss/Train\", loss.item(), batch_idx)\n",
    "            batch_idx += 1\n",
    "            if batch_idx % 25 == 0:\n",
    "                print('Evaluating on valiudation set')\n",
    "                # evaluate the validation set performance\n",
    "                val_loss, val_acc = evaluate(model, val_loader)\n",
    "                writer.add_scalar(\"Loss/Val\", val_loss, batch_idx)\n",
    "                writer.add_scalar(\"Accuracy/Val\", val_acc, batch_idx)\n",
    "\n",
    "        scheduler.step()\n",
    "    # evaluate the final test set performance\n",
    "    \n",
    "    print('Evaluating on test set')\n",
    "    test_loss = evaluate(model, test_loader)\n",
    "    # writer.add_scalar(\"Loss/Test\", test_loss, batch_idx)\n",
    "    model.test_loss = test_loss\n",
    "    \n",
    "    return model   # return trained model\n",
    "    \n",
    "\n",
    "def evaluate(model, dataloader):\n",
    "    losses = []\n",
    "    correct = 0\n",
    "    n_examples = 0\n",
    "    for batch in dataloader:\n",
    "        features, labels = batch\n",
    "        prediction = model(features)\n",
    "        loss = F.cross_entropy(prediction, labels)\n",
    "        losses.append(loss.detach())\n",
    "        correct += torch.sum(torch.argmax(prediction, dim=1) == labels)\n",
    "        n_examples += len(labels)\n",
    "    avg_loss = np.mean(losses)\n",
    "    accuracy = correct / n_examples\n",
    "    print(\"Loss:\", avg_loss, \"Accuracy:\", accuracy.detach().numpy())\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "trained_model=train(\n",
    "                model,\n",
    "                train_loader,\n",
    "                val_loader,\n",
    "                test_loader,\n",
    "                epochs=5,\n",
    "                lr=0.001,\n",
    "                optimiser=torch.optim.AdamW\n",
    "                )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's view training performance in `tensorboard`. Run the cell below to open a `tensorboard` instance, then select `time series` from the dropdown, and press refresh to view the training loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir runs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's re-run our prediction code to see how the trained model performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features,label=test_set[1]\n",
    "features=features.unsqueeze(0)\n",
    "model.eval()\n",
    "outputs=model(features)\n",
    "\n",
    "dummy, pred = torch.max(outputs, 1)\n",
    "print(\"Prediction label: \", pred.item())\n",
    "class_label = dataset.idx_to_city_name[pred.item()]\n",
    "print(\"Prediction category: \", class_label)\n",
    "\n",
    "print(\"target label:\", label )\n",
    "target_label=dataset.idx_to_city_name[label]\n",
    "print( \"target city \", target_label )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "e36d4b688d7e3685ae8ad6703c0e99019531dd9f05b6e8f8c82292a1f759bcdc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
